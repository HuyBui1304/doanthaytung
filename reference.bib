
@misc{noauthor_umap_nodate,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction} — umap 0.5.8 documentation},
	url = {https://umap-learn.readthedocs.io/en/latest/},
	urldate = {2025-04-15},
	file = {UMAP\: Uniform Manifold Approximation and Projection for Dimension Reduction — umap 0.5.8 documentation:/Users/huy/Zotero/storage/MVBA74A4/latest.html:text/html},
}

@misc{noauthor_t-test_nodate,
	title = {t-{Test}, {Chi}-{Square}, {ANOVA}, {Regression}, {Correlation}...},
	url = {https://datatab.net/tutorial/logistic-regression},
	abstract = {Webapp for statistical data analysis.},
	language = {en},
	urldate = {2025-04-15},
	file = {Snapshot:/Users/huy/Zotero/storage/9VIGT8ZB/logistic-regression.html:text/html},
}

@misc{noauthor_complete_2020,
	title = {A {Complete} {Guide} to {Decision} {Trees} {\textbar} {Paperspace} {Blog}},
	url = {https://blog.paperspace.com/decision-trees/},
	abstract = {This is a 2020 guide to decision trees, which are foundational to many machine learning algorithms including random forests and various ensemble methods.},
	language = {en},
	urldate = {2025-04-15},
	journal = {Paperspace by DigitalOcean Blog},
	month = feb,
	year = {2020},
	file = {Snapshot:/Users/huy/Zotero/storage/MZUE563W/decision-trees.html:text/html},
}

@misc{noauthor_guide_nodate,
	title = {Guide to {K}-{Nearest} {Neighbors} ({KNN}) {Algorithm} [2025 {Edition}]},
	url = {https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/},
	urldate = {2025-04-15},
	file = {Guide to K-Nearest Neighbors (KNN) Algorithm [2025 Edition]:/Users/huy/Zotero/storage/SBBWPTPG/introduction-k-neighbours-algorithm-clustering.html:text/html},
}

@article{jakkula_tutorial_nodate,
	title = {Tutorial on {Support} {Vector} {Machine} ({SVM})},
	abstract = {In this tutorial we present a brief introduction to SVM, and we discuss about SVM from published papers, workshop materials \& material collected from books and material available online on the World Wide Web. In the beginning we try to define SVM and try to talk as why SVM, with a brief overview of statistical learning theory. The mathematical formulation of SVM is presented, and theory for the implementation of SVM is briefly discussed. Finally some conclusions on SVM and application areas are included. Support Vector Machines (SVMs) are competing with Neural Networks as tools for solving pattern recognition problems. This tutorial assumes you are familiar with concepts of Linear Algebra, real analysis and also understand the working of neural networks and have some background in AI.},
	language = {en},
	author = {Jakkula, Vikramaditya},
	file = {PDF:/Users/huy/Zotero/storage/I9V2AZ8T/Jakkula - Tutorial on Support Vector Machine (SVM).pdf:application/pdf},
}

@misc{noauthor_random_nodate,
	title = {Random {Forest} {Algorithm} in {Machine} {Learning}},
	url = {https://www.simplilearn.com/tutorials/machine-learning-tutorial/random-forest-algorithm},
	abstract = {Random Forest Algorithm operates by constructing multiple decision trees. Learn the important Random Forest algorithm terminologies and use cases. Read on!},
	language = {en-US},
	urldate = {2025-04-15},
	journal = {Simplilearn.com},
	file = {Snapshot:/Users/huy/Zotero/storage/MPPCPPUS/random-forest-algorithm.html:text/html},
}

@misc{noauthor_110_nodate,
	title = {1.10. {Decision} {Trees}},
	url = {https://scikit-learn/stable/modules/tree.html},
	abstract = {Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning s...},
	language = {en},
	urldate = {2025-04-15},
	journal = {scikit-learn},
	file = {Snapshot:/Users/huy/Zotero/storage/ZMVV6AVE/tree.html:text/html},
}

@misc{noauthor_14_nodate,
	title = {1.4. {Support} {Vector} {Machines}},
	url = {https://scikit-learn/stable/modules/svm.html},
	abstract = {Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high ...},
	language = {en},
	urldate = {2025-04-15},
	journal = {scikit-learn},
	file = {Snapshot:/Users/huy/Zotero/storage/THBU8W8K/svm.html:text/html},
}

@misc{noauthor_human_nodate,
	title = {Human {Activity} {Recognition} with {Smartphones}},
	url = {https://www.kaggle.com/datasets/uciml/human-activity-recognition-with-smartphones},
	abstract = {Recordings of 30 study participants performing activities of daily living},
	language = {en},
	urldate = {2025-04-15},
	file = {Snapshot:/Users/huy/Zotero/storage/99LJBQRE/human-activity-recognition-with-smartphones.html:text/html},
}

@misc{noauthor_nearest_nodate,
	title = {Nearest neighbor pattern classification {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/1053964},
	urldate = {2025-04-15},
	file = {Nearest neighbor pattern classification | IEEE Journals & Magazine | IEEE Xplore:/Users/huy/Zotero/storage/NNTXTIHB/1053964.html:text/html},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2025-04-15},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {Artificial Intelligence, classification, ensemble, regression},
	pages = {5--32},
	file = {Full Text PDF:/Users/huy/Zotero/storage/PED63IZJ/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@misc{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv:1802.03426 [stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/huy/Zotero/storage/QSKTPULL/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projection for Dimension Reduction.pdf:application/pdf;Snapshot:/Users/huy/Zotero/storage/IATRB7DJ/1802.html:text/html},
}

@incollection{noauthor_frontmatter_2013,
	title = {{FrontMatter}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.fmatter},
	abstract = {This chapter contains sections titled: Half-Title Series Page Title Copyright Dedication Contents Preface to the Third Edition},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.fmatter},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.fmatter},
	pages = {i--xvi},
	file = {Full Text PDF:/Users/huy/Zotero/storage/7RZRYWMY/2013 - FrontMatter.pdf:application/pdf;Snapshot:/Users/huy/Zotero/storage/KJCR2CD7/9781118548387.html:text/html},
}

@incollection{noauthor_introduction_2013,
	title = {Introduction to the {Logistic} {Regression} {Model}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch1},
	abstract = {This chapter contains sections titled: Introduction Fitting the Logistic Regression Model Testing for the Significance of the Coefficients Confidence Interval Estimation Other Estimation Methods Data Sets Used in Examples and Exercises Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch1},
	note = {Section: 1
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch1},
	pages = {1--33},
	file = {Snapshot:/Users/huy/Zotero/storage/RYA5YGQN/9781118548387.html:text/html},
}

@incollection{noauthor_multiple_2013,
	title = {The {Multiple} {Logistic} {Regression} {Model}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch2},
	abstract = {This chapter contains sections titled: Introduction The Multiple Logistic Regression Model Fitting the Multiple Logistic Regression Model Testing for the Significance of the Model Confidence Interval Estimation Other Estimation Methods Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch2},
	note = {Section: 2
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch2},
	pages = {35--47},
	file = {Snapshot:/Users/huy/Zotero/storage/TLSUVE5W/9781118548387.html:text/html},
}

@incollection{noauthor_interpretation_2013,
	title = {Interpretation of the {Fitted} {Logistic} {Regression} {Model}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch3},
	abstract = {This chapter contains sections titled: Introduction Dichotomous Independent Variable Polychotomous Independent Variable Continuous Independent Variable Multivariable Models Presentation and Interpretation of the Fitted Values A Comparison of Logistic Regression and Stratified Analysis for 2 × 2 Tables Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch3},
	note = {Section: 3
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch3},
	pages = {49--88},
	file = {Snapshot:/Users/huy/Zotero/storage/MG7CJXQN/9781118548387.html:text/html},
}

@incollection{noauthor_model-building_2013,
	title = {Model-{Building} {Strategies} and {Methods} for {Logistic} {Regression}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch4},
	abstract = {This chapter contains sections titled: Introduction Purposeful Selection of Covariates Other Methods for Selecting Covariates Numerical Problems Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch4},
	note = {Section: 4
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch4},
	pages = {89--151},
	file = {Snapshot:/Users/huy/Zotero/storage/HYGNVE8U/9781118548387.html:text/html},
}

@incollection{noauthor_assessing_2013,
	title = {Assessing the {Fit} of the {Model}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch5},
	abstract = {This chapter contains sections titled: Introduction Summary Measures of Goodness of Fit Logistic Regression Diagnostics Assessment of Fit Via External Validation Interpretation and Presentation of the Results from a Fitted Logistic Regression Model Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch5},
	note = {Section: 5
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch5},
	pages = {153--225},
	file = {Snapshot:/Users/huy/Zotero/storage/EJML259R/9781118548387.html:text/html},
}

@incollection{noauthor_application_2013,
	title = {Application of {Logistic} {Regression} with {Different} {Sampling} {Models}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch6},
	abstract = {This chapter contains sections titled: Introduction Cohort Studies Case-Control Studies Fitting Logistic Regression Models to Data From Complex Sample Surveys Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch6},
	note = {Section: 6
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch6},
	pages = {227--242},
	file = {Snapshot:/Users/huy/Zotero/storage/DE38UPVC/9781118548387.html:text/html},
}

@incollection{noauthor_logistic_2013,
	title = {Logistic {Regression} for {Matched} {Case}-{Control} {Studies}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch7},
	abstract = {This chapter contains sections titled: Introduction Methods For Assessment of Fit in a 1−M Matched Study An Example Using the Logistic Regression Model in a 1–1 Matched Study Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch7},
	note = {Section: 7
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch7},
	pages = {243--268},
	file = {Snapshot:/Users/huy/Zotero/storage/7PIGG3XT/9781118548387.html:text/html},
}

@incollection{noauthor_logistic_2013-1,
	title = {Logistic {Regression} {Models} for {Multinomial} and {Ordinal} {Outcomes}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch8},
	abstract = {This chapter contains sections titled: The Multinomial Logistic Regression Model Ordinal Logistic Regression Models Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch8},
	note = {Section: 8
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch8},
	pages = {269--311},
	file = {Snapshot:/Users/huy/Zotero/storage/VTWRRZZG/9781118548387.html:text/html},
}

@incollection{noauthor_logistic_2013-2,
	title = {Logistic {Regression} {Models} for the {Analysis} of {Correlated} {Data}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch9},
	abstract = {This chapter contains sections titled: Introduction Logistic Regression Models for the Analysis of Correlated Data Estimation Methods for Correlated Data Logistic Regression Models Interpretation of Coefficients from Logistic Regression Models for the Analysis of Correlated Data An Example of Logistic Regression Modeling with Correlated Data Assessment of Model Fit Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch9},
	note = {Section: 9
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch9},
	pages = {313--376},
	file = {Snapshot:/Users/huy/Zotero/storage/ECZLJYTG/9781118548387.html:text/html},
}

@incollection{noauthor_special_2013,
	title = {Special {Topics}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.ch10},
	abstract = {This chapter contains sections titled: Introduction Application of Propensity Score Methods in Logistic Regression Modeling Exact Methods for Logistic Regression Models Missing Data Sample Size Issues When Fitting Logistic Regression Models Bayesian Methods for Logistic Regression Other Link Functions for Binary Regression Models Mediation More About Statistical Interaction Exercises},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.ch10},
	note = {Section: 10
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.ch10},
	pages = {377--457},
	file = {Snapshot:/Users/huy/Zotero/storage/2MZW2N5J/9781118548387.html:text/html},
}

@incollection{noauthor_references_2013,
	title = {References},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.refs},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.refs},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.refs},
	pages = {459--478},
	file = {Full Text PDF:/Users/huy/Zotero/storage/BJIUYAJP/2013 - References.pdf:application/pdf;Snapshot:/Users/huy/Zotero/storage/8Q522RJP/9781118548387.html:text/html},
}

@incollection{noauthor_index_2013,
	title = {Index},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.index},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.index},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.index},
	pages = {479--500},
	file = {Full Text PDF:/Users/huy/Zotero/storage/T5XIQISB/2013 - Index.pdf:application/pdf;Snapshot:/Users/huy/Zotero/storage/7SNBIC3T/9781118548387.html:text/html},
}

@incollection{noauthor_wiley_2013,
	title = {Wiley {Series} in {Probability} and {Statistics}},
	isbn = {978-1-118-54838-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118548387.scard},
	language = {en},
	urldate = {2025-04-15},
	booktitle = {Applied {Logistic} {Regression}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2013},
	doi = {10.1002/9781118548387.scard},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118548387.scard},
	pages = {501--510},
	file = {Full Text PDF:/Users/huy/Zotero/storage/2XBVAYR6/2013 - Wiley Series in Probability and Statistics.pdf:application/pdf;Snapshot:/Users/huy/Zotero/storage/ZBC478HZ/9781118548387.html:text/html},
}
